{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**LLM**"
      ],
      "metadata": {
        "id": "xaoHWL07SB9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QBuqBC1GRro0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8166430b-2d98-4e06-a74c-a902ed68128b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install groq colorama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "7nRz9Ir0R-GI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from groq import Groq\n",
        "from IPython.display import display_markdown\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import Callable\n",
        "import time\n",
        "\n",
        "from colorama import Fore\n",
        "from colorama import Style"
      ],
      "metadata": {
        "id": "h9FBRyOeSBSg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
        "# MODEL = \"deepseek-r1-distill-llama-70b\"\n",
        "MODEL = \"llama-3.3-70b-versatile\"\n",
        "GROQ_CLIENT =  Groq(api_key=groq_api_key)"
      ],
      "metadata": {
        "id": "6nV5z2LZSyJ4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Function --> tool**"
      ],
      "metadata": {
        "id": "WTyjW9fuSQ3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_current_weather(location: str, unit: str):\n",
        "\t\"\"\"\n",
        "\tGet the current weather in a given location\n",
        "\n",
        "\tlocation (str): The city and state, e.g. Madrid, Barcelona\n",
        "\tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\"\n",
        "\t\"\"\"\n",
        "\tif location == \"Madrid\":\n",
        "\t\treturn json.dumps({\"temperature\": 25, \"unit\": unit})\n",
        "\n",
        "\telse:\n",
        "\t\treturn json.dumps({\"temperature\": 58, \"unit\": unit})"
      ],
      "metadata": {
        "id": "20aoZR-ySFge"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_weather(location=\"Madrid\", unit=\"celsius\")\n"
      ],
      "metadata": {
        "id": "3VfIGFMzSU4t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f69c2eb4-67ca-4e29-ec3d-5479b67f62fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"temperature\": 25, \"unit\": \"celsius\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**tool call agent**"
      ],
      "metadata": {
        "id": "bnnhNM6HTAF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**system prompt**\n",
        "manually pass the tool signature to the prompt"
      ],
      "metadata": {
        "id": "wLDQRXaESc0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the System Prompt as a constant\n",
        "\n",
        "TOOL_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools> {\n",
        "    \"name\": \"get_current_weather\",\n",
        "    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n",
        "    \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"str\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "                \"type\": \"str\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "</tools>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u9AZOZMVSX21"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chat History**\n",
        "1. Tool chat history\n",
        "2. agent chat history"
      ],
      "metadata": {
        "id": "Kf4qsUxMS2gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_chat_history = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": TOOL_SYSTEM_PROMPT\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "agent_chat_history = []\n",
        "\n",
        "# User Query\n",
        "user_msg = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What's the current temperature in Madrid, in Celsius?\"\n",
        "}\n",
        "\n",
        "\n",
        "tool_chat_history.append(user_msg)\n",
        "agent_chat_history.append(user_msg)\n"
      ],
      "metadata": {
        "id": "5GRMQKmiSXze"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=tool_chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "0qf4zIT4SXtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0a1b6b-6a1c-4f6c-8597-84565e75c5c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tool_call>\n",
            "{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}}\n",
            "</tool_call>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tool parser**"
      ],
      "metadata": {
        "id": "THpPQQDyTaVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tool_call_str(tool_call_str: str):\n",
        "    pattern = r'</?tool_call>'\n",
        "    clean_tags = re.sub(pattern, '', tool_call_str)\n",
        "\n",
        "    try:\n",
        "        tool_call_json = json.loads(clean_tags)\n",
        "        return tool_call_json\n",
        "    except json.JSONDecodeError:\n",
        "        return clean_tags\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        return \"There was some error parsing the Tool's output\""
      ],
      "metadata": {
        "id": "Sxf0QXVUTWqz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_output = parse_tool_call_str(output)\n",
        "parsed_output"
      ],
      "metadata": {
        "id": "SGTZxY1DTc5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75077b2-8f81-40ec-d9d5-d8bded781106"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_weather',\n",
              " 'arguments': {'location': 'Madrid', 'unit': 'celsius'}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Observation**\n",
        "observation is the output of tool runner"
      ],
      "metadata": {
        "id": "mHAKBYaSTgX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_current_weather(**parsed_output[\"arguments\"])\n",
        "result"
      ],
      "metadata": {
        "id": "4dVyEdvYTgAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1e88c250-60eb-4bd9-8686-e7ef361750cb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"temperature\": 25, \"unit\": \"celsius\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Appending observation to agent history**"
      ],
      "metadata": {
        "id": "uMsjUJDfTvan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chat_history.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": f\"Observation: {result}\"\n",
        "})"
      ],
      "metadata": {
        "id": "qQVj2M5YTrFh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Final step --> llm call with the agent history**"
      ],
      "metadata": {
        "id": "uc0TqNdMT1zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_CLIENT.chat.completions.create(\n",
        "    messages=agent_chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content"
      ],
      "metadata": {
        "id": "U9Mt3K3mT0xc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a9fad6aa-246d-4b04-cb3c-f3d343807545"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature in Madrid is 25°C.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**All together**"
      ],
      "metadata": {
        "id": "sLHmyx_1UJaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Completion from llm**"
      ],
      "metadata": {
        "id": "CwkKghg9VKcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def completions_create(client, messages: list, model: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a request to the client's `completions.create` method to interact with the language model.\n",
        "\n",
        "    Args:\n",
        "        client (Groq): The Groq client object\n",
        "        messages (list[dict]): A list of message objects containing chat history for the model.\n",
        "        model (str): The model to use for generating tool calls and responses.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the model's response.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(messages=messages, model=model)\n",
        "    return str(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "YsUhtkj8VTyt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**prompt**"
      ],
      "metadata": {
        "id": "r5KSYMFIVV44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOOL_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call>\n",
        "XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>,  \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "402iQITYVaCi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Builds a structured prompt that includes the role and content.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The actual content of the prompt.\n",
        "        role (str): The role of the speaker (e.g., user, assistant).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the structured prompt.\n",
        "    \"\"\"\n",
        "    if tag:\n",
        "        prompt = f\"<{tag}>{prompt}</{tag}>\"\n",
        "    return {\"role\": role, \"content\": prompt}"
      ],
      "metadata": {
        "id": "kJjd6gizVdfD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chat history**"
      ],
      "metadata": {
        "id": "f1DHKnoBVt9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatHistory(list):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        if messages is None:\n",
        "            messages = []\n",
        "\n",
        "        super().__init__(messages)\n",
        "        self.total_length = total_length\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(0)\n",
        "        super().append(msg)\n"
      ],
      "metadata": {
        "id": "595m23IvVwCe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_chat_history(history: list, msg: str, role: str):\n",
        "    \"\"\"\n",
        "    Updates the chat history by appending the latest response.\n",
        "\n",
        "    Args:\n",
        "        history (list): The list representing the current chat history.\n",
        "        msg (str): The message to append.\n",
        "        role (str): The role type (e.g. 'user', 'assistant', 'system')\n",
        "    \"\"\"\n",
        "    history.append(build_prompt_structure(prompt=msg, role=role))\n"
      ],
      "metadata": {
        "id": "Nngw3j_pV3jL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tool utils**"
      ],
      "metadata": {
        "id": "EsPzdMGuXQRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Function json output**"
      ],
      "metadata": {
        "id": "wlw78XtSXozp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<tools> {\n",
        "    \"name\": \"get_current_weather\",\n",
        "    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n",
        "    \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"str\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "                \"type\": \"str\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "</tools>"
      ],
      "metadata": {
        "id": "lNHHHeu5Z4Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fn_signature(fn: Callable) -> dict:\n",
        "    \"\"\"\n",
        "    Generates the signature for a given function.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function whose signature needs to be extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the function's name, description,\n",
        "              and parameter types.\n",
        "    \"\"\"\n",
        "    fn_signature: dict = {\n",
        "        \"name\": fn.__name__,\n",
        "        \"description\": fn.__doc__,\n",
        "        \"parameters\": {\"properties\": {}},\n",
        "    }\n",
        "    schema = {\n",
        "        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n",
        "    }\n",
        "    fn_signature[\"parameters\"][\"properties\"] = schema\n",
        "    return fn_signature\n"
      ],
      "metadata": {
        "id": "9xT0NfOtXb4C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_fn_signature(get_current_weather)"
      ],
      "metadata": {
        "id": "Mm9VwNRDAJCx",
        "outputId": "b430d6ef-6b80-434e-e9ec-3172ba0bea3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_weather',\n",
              " 'description': '\\n\\tGet the current weather in a given location\\n\\n\\tlocation (str): The city and state, e.g. Madrid, Barcelona\\n\\tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\"\\n\\t',\n",
              " 'parameters': {'properties': {'location': {'type': 'str'},\n",
              "   'unit': {'type': 'str'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**tool class and decorator**"
      ],
      "metadata": {
        "id": "418lMa4DXud0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tool:\n",
        "    \"\"\"\n",
        "    A class representing a tool that wraps a callable and its signature.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the tool (function).\n",
        "        fn (Callable): The function that the tool represents.\n",
        "        fn_signature (str): JSON string representation of the function's signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, fn: Callable, fn_signature: str):\n",
        "        self.name = name  # tool name\n",
        "        self.fn = fn      # callable function\n",
        "        self.fn_signature = fn_signature  # output from the previous function\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fn_signature\n",
        "\n",
        "    def run(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Executes the tool (function) with provided arguments.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Keyword arguments passed to the function.\n",
        "\n",
        "        Returns:\n",
        "            The result of the function call.\n",
        "        \"\"\"\n",
        "        return self.fn(**kwargs)"
      ],
      "metadata": {
        "id": "CgqzqfMYXzPS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tool(fn: Callable):\n",
        "    \"\"\"\n",
        "    A decorator that wraps a function into a Tool object.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function to be wrapped.\n",
        "\n",
        "    Returns:\n",
        "        Tool: A Tool object containing the function, its name, and its signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper():\n",
        "        fn_signature = get_fn_signature(fn)\n",
        "        return Tool(\n",
        "            name=fn_signature.get(\"name\"),\n",
        "            fn=fn,\n",
        "            fn_signature=json.dumps(fn_signature)\n",
        "        )\n",
        "\n",
        "    return wrapper()"
      ],
      "metadata": {
        "id": "7FhHRBPjX6H2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Arguments Validator**"
      ],
      "metadata": {
        "id": "a8VwPzuzX87V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_arguments(tool_call: dict, tool_signature: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Validates and converts arguments in the input dictionary to match the expected types.\n",
        "\n",
        "    Args:\n",
        "        tool_call (dict): A dictionary containing the arguments passed to the tool.\n",
        "        tool_signature (dict): The expected function signature and parameter types.\n",
        "\n",
        "    Returns:\n",
        "        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n",
        "    \"\"\"\n",
        "    properties = tool_signature[\"parameters\"][\"properties\"]\n",
        "\n",
        "    # TODO: This is overly simplified but enough for simple Tools.\n",
        "    type_mapping = {\n",
        "        \"int\": int,\n",
        "        \"str\": str,\n",
        "        \"bool\": bool,\n",
        "        \"float\": float,\n",
        "    }\n",
        "\n",
        "    for arg_name, arg_value in tool_call[\"arguments\"].items():\n",
        "        expected_type = properties[arg_name].get(\"type\")\n",
        "\n",
        "        # check if output arg value from the model != expected\n",
        "        if not isinstance(arg_value, type_mapping[expected_type]):\n",
        "            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n",
        "\n",
        "    return tool_call"
      ],
      "metadata": {
        "id": "jWj49VGWYCzY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tag exctractor**"
      ],
      "metadata": {
        "id": "IH4LEOkuHsC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TagContentResult:\n",
        "    \"\"\"\n",
        "    A data class to represent the result of extracting tag content.\n",
        "\n",
        "    Attributes:\n",
        "        content (List[str]): A list of strings containing the content found between the specified tags.\n",
        "        found (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "\n",
        "    content: list[str]\n",
        "    found: bool\n",
        "\n",
        "\n",
        "def extract_tag_content(text: str, tag: str) -> TagContentResult:\n",
        "    \"\"\"\n",
        "    Extracts all content enclosed by specified tags (e.g., <thought>, <response>, etc.).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input string containing multiple potential tags.\n",
        "        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the following keys:\n",
        "            - 'content' (list): A list of strings containing the content found between the specified tags.\n",
        "            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "    # Build the regex pattern dynamically to find multiple occurrences of the tag\n",
        "    tag_pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
        "\n",
        "    # Use findall to capture all content between the specified tag\n",
        "    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Return the dataclass instance with the result\n",
        "    return TagContentResult(\n",
        "        content=[content.strip() for content in matched_contents],\n",
        "        found=bool(matched_contents),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "t4FLAdkmJPo1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tool Agent class**"
      ],
      "metadata": {
        "id": "CoQZNthRU-ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToolAgent:\n",
        "    \"\"\"\n",
        "    The ToolAgent class represents an agent that can interact with a language model and use tools\n",
        "    to assist with user queries. It generates function calls based on user input, validates arguments,\n",
        "    and runs the respective tools.\n",
        "\n",
        "    Attributes:\n",
        "        tools (Tool | list[Tool]): A list of tools available to the agent.\n",
        "        model (str): The model to be used for generating tool calls and responses.\n",
        "        client (Groq): The Groq client used to interact with the language model.\n",
        "        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tools: Tool | list[Tool], model: str = \"llama-3.3-70b-versatile\") -> None:\n",
        "        self.client = Groq(api_key=groq_api_key)\n",
        "        self.model = model\n",
        "        self.tools = tools if isinstance(tools, list) else [tools] # force to be a list\n",
        "        self.tools_dict = {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def add_tool_signatures(self) -> str:\n",
        "        \"\"\"\n",
        "        Collects the function signatures of all available tools.\n",
        "\n",
        "        Returns:\n",
        "            str: A concatenated string of all tool function signatures in JSON format.\n",
        "        \"\"\"\n",
        "        return \"\".join([tool.fn_signature for tool in self.tools])\n",
        "\n",
        "    def process_tool_calls(self, tool_calls_content: list) -> dict:\n",
        "        \"\"\"\n",
        "        Processes each tool call, validates arguments, executes the tools, and collects results.\n",
        "\n",
        "        Args:\n",
        "            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n",
        "        \"\"\"\n",
        "        observations = {}\n",
        "        # tool call content is in json format returning from the model\n",
        "        # we want : tool name , tool\n",
        "        # we have tool dict {\"tool_name\" : tool }\n",
        "        for tool_call_str in tool_calls_content:\n",
        "            tool_call = json.loads(tool_call_str)\n",
        "            tool_name = tool_call[\"name\"]\n",
        "            tool = self.tools_dict[tool_name]\n",
        "\n",
        "            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n",
        "\n",
        "            # Validate and execute the tool call\n",
        "            validated_tool_call = validate_arguments(\n",
        "                tool_call, json.loads(tool.fn_signature)\n",
        "            )\n",
        "            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n",
        "\n",
        "            result = tool.run(**validated_tool_call[\"arguments\"])\n",
        "            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n",
        "\n",
        "            # Store the result using the tool call ID\n",
        "            observations[validated_tool_call[\"id\"]] = result\n",
        "\n",
        "        return observations\n",
        "\n",
        "    def run(self, user_msg: str) -> str:\n",
        "        \"\"\"\n",
        "        Handles the full process of interacting with the language model and executing a tool based on user input.\n",
        "\n",
        "        Args:\n",
        "            user_msg (str): The user's message that prompts the tool agent to act.\n",
        "\n",
        "        Returns:\n",
        "            str: The final output after executing the tool and generating a response from the model.\n",
        "        \"\"\"\n",
        "        # build user query prompt\n",
        "        user_prompt = build_prompt_structure(prompt=user_msg, role=\"user\")\n",
        "\n",
        "        # append system msg and user query to tool chat history\n",
        "        tool_chat_history = ChatHistory(\n",
        "            [\n",
        "                build_prompt_structure(\n",
        "                    prompt=TOOL_SYSTEM_PROMPT % self.add_tool_signatures(),\n",
        "                    role=\"system\",\n",
        "                ),\n",
        "                user_prompt,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # append user query to the agent chat hsitory\n",
        "        agent_chat_history = ChatHistory([user_prompt])\n",
        "\n",
        "        # tall call output --> tool name + tool args\n",
        "        tool_call_response = completions_create(\n",
        "            self.client, messages=tool_chat_history, model=self.model\n",
        "        )\n",
        "\n",
        "        # tool_call_response from the llm model  <tool_call> </tool_call>\n",
        "        # also pass the tag = <tool_call>\n",
        "        # Extract_tag_content returns:\n",
        "        # - 'content' (list): A list of strings containing the content found between the specified tags.\n",
        "        # - 'found' (bool): A flag indicating whether any content was found for the given tag.\n",
        "        tool_calls = extract_tag_content(str(tool_call_response), \"tool_call\")\n",
        "\n",
        "        # getting observation to add to the agent history\n",
        "        if tool_calls.found:\n",
        "            observations = self.process_tool_calls(tool_calls.content)\n",
        "            update_chat_history(\n",
        "                agent_chat_history, f'f\"Observation: {observations}\"', \"user\"\n",
        "            )\n",
        "\n",
        "        # final step to generate final response from the observation\n",
        "        return completions_create(self.client, agent_chat_history, self.model)"
      ],
      "metadata": {
        "id": "tpt58WyRUKuW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Testing**"
      ],
      "metadata": {
        "id": "yGTZtCIoPQIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tool to use**"
      ],
      "metadata": {
        "id": "XBnlgBItQGiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "def fetch_top_hacker_news_stories(top_n: int):\n",
        "    \"\"\"\n",
        "    Fetch the top stories from Hacker News.\n",
        "\n",
        "    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\n",
        "    Each story contains the title, URL, score, author, and time of submission. The data is fetched\n",
        "    from the official Firebase Hacker News API, which returns story details in JSON format.\n",
        "\n",
        "    Args:\n",
        "        top_n (int): The number of top stories to retrieve.\n",
        "    \"\"\"\n",
        "    top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(top_stories_url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "        # Get the top story IDs\n",
        "        top_story_ids = response.json()[:top_n]\n",
        "\n",
        "        top_stories = []\n",
        "\n",
        "        # For each story ID, fetch the story details\n",
        "        for story_id in top_story_ids:\n",
        "            story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'\n",
        "            story_response = requests.get(story_url)\n",
        "            story_response.raise_for_status()  # Check for HTTP errors\n",
        "            story_data = story_response.json()\n",
        "\n",
        "            # Append the story title and URL (or other relevant info) to the list\n",
        "            top_stories.append({\n",
        "                'title': story_data.get('title', 'No title'),\n",
        "                'url': story_data.get('url', 'No URL available'),\n",
        "            })\n",
        "\n",
        "        return json.dumps(top_stories)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "e-UemoHAaLgH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hn_tool = tool(fetch_top_hacker_news_stories)\n",
        "hn_tool"
      ],
      "metadata": {
        "id": "IHkzEguBPXQx",
        "outputId": "46760891-7632-4970-84a6-d4957baf048d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Tool at 0x7c82a2cc4690>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hn_tool.name\n"
      ],
      "metadata": {
        "id": "qlSYyh0GPe7j",
        "outputId": "31331b28-08f7-477c-d7d7-fbb61a227efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fetch_top_hacker_news_stories'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hn_tool.fn_signature"
      ],
      "metadata": {
        "id": "mzx_R0O0PlEv",
        "outputId": "fe4ae2e3-f028-4942-9065-46e632fe4b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"fetch_top_hacker_news_stories\", \"description\": \"\\\\n    Fetch the top stories from Hacker News.\\\\n\\\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \\\\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched \\\\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\\\n\\\\n    Args:\\\\n        top_n (int): The number of top stories to retrieve.\\\\n    \", \"parameters\": {\"properties\": {\"top_n\": {\"type\": \"int\"}}}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json.loads(hn_tool.fn_signature)\n"
      ],
      "metadata": {
        "id": "gEcI_SZ8PfKU",
        "outputId": "f2d13811-d7c1-4fac-c75b-ac72e4e9e655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'fetch_top_hacker_news_stories',\n",
              " 'description': '\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched \\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    ',\n",
              " 'parameters': {'properties': {'top_n': {'type': 'int'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Agent instant**"
      ],
      "metadata": {
        "id": "vuR582nXQJcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_agent = ToolAgent(tools=[hn_tool])\n"
      ],
      "metadata": {
        "id": "UqPbeNocPi1q"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = tool_agent.run(user_msg=\"Tell me the top 5 Hacker News stories right now\")\n"
      ],
      "metadata": {
        "id": "4dofUtz9P3gG",
        "outputId": "9a2a964c-f93f-4758-a97f-6cd097586314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\n",
            "Using Tool: fetch_top_hacker_news_stories\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'fetch_top_hacker_news_stories', 'arguments': {'top_n': 5}, 'id': 1}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "[{\"title\": \"Taking a $15 Casio F91W 5km underwater\", \"url\": \"https://www.watchesofespionage.com/blogs/woe-dispatch/casio-f91w-diving-underwater-pressure-test\"}, {\"title\": \"Images Reveal Exocomets Around 74 Nearby Stars\", \"url\": \"https://skyandtelescope.org/astronomy-news/new-images-reveal-exocomets-around-74-nearby-stars/\"}, {\"title\": \"MillenniumDB: A graph database engine using domain graphs\", \"url\": \"https://github.com/MillenniumDB/MillenniumDB\"}, {\"title\": \"Building a Mesh Using Spherical Embedding\", \"url\": \"https://andrews.wiki/spherical-mesh\"}, {\"title\": \"Hydro: Distributed Programming Framework for Rust\", \"url\": \"https://hydro.run/docs/hydro/\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "OpbEpfo8QrWf",
        "outputId": "e9bab3c8-812e-4b16-b555-41a332907493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 5 Hacker News stories right now are:\n",
            "\n",
            "1. **Taking a $15 Casio F91W 5km underwater**: A blog post about testing the water resistance of a cheap Casio watch, pushing it to its limits 5km underwater. (https://www.watchesofespionage.com/blogs/woe-dispatch/casio-f91w-diving-underwater-pressure-test)\n",
            "\n",
            "2. **Images Reveal Exocomets Around 74 Nearby Stars**: An article from Sky & Telescope about new images that have revealed exocomets orbiting around 74 nearby stars, shedding more light on our understanding of the universe. (https://skyandtelescope.org/astronomy-news/new-images-reveal-exocomets-around-74-nearby-stars/)\n",
            "\n",
            "3. **MillenniumDB: A graph database engine using domain graphs**: An open-source graph database engine on GitHub that utilizes domain graphs for efficient data storage and querying. (https://github.com/MillenniumDB/MillenniumDB)\n",
            "\n",
            "4. **Building a Mesh Using Spherical Embedding**: A wiki article about creating a mesh using spherical embedding, a technique for generating 3D models from spherical coordinates. (https://andrews.wiki/spherical-mesh)\n",
            "\n",
            "5. **Hydro: Distributed Programming Framework for Rust**: A documentation page for Hydro, a distributed programming framework designed for the Rust programming language, allowing developers to build scalable and concurrent systems. (https://hydro.run/docs/hydro/)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PASbk-93RBmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}