{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**LLM**"
      ],
      "metadata": {
        "id": "8pApXG7XkuJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XWesa9ua4B6",
        "outputId": "d5b1a1c8-9734-48bb-9b94-b19c8ec5065b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.16.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, groq\n",
            "Successfully installed colorama-0.4.6 groq-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq colorama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')"
      ],
      "metadata": {
        "id": "4RY-lfeDbEIb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from groq import Groq\n",
        "from IPython.display import display_markdown\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "from colorama import Fore\n",
        "from colorama import Style"
      ],
      "metadata": {
        "id": "dMpQ9Y4vbEFG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"llama-3.3-70b-versatile\"\n",
        "GROQ_CLIENT =  Groq(api_key=groq_api_key)\n"
      ],
      "metadata": {
        "id": "kqiohEXFbECz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tool utils**"
      ],
      "metadata": {
        "id": "qPfoQqnollm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fn_signature(fn: Callable) -> dict:\n",
        "    \"\"\"\n",
        "    Generates the signature for a given function.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function whose signature needs to be extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the function's name, description,\n",
        "              and parameter types.\n",
        "    \"\"\"\n",
        "    fn_signature: dict = {\n",
        "        \"name\": fn.__name__,\n",
        "        \"description\": fn.__doc__,\n",
        "        \"parameters\": {\"properties\": {}},\n",
        "    }\n",
        "    schema = {\n",
        "        k: {\"type\": v.__name__} for k, v in fn.__annotations__.items() if k != \"return\"\n",
        "    }\n",
        "    fn_signature[\"parameters\"][\"properties\"] = schema\n",
        "    return fn_signature\n",
        "\n",
        "\n",
        "def validate_arguments(tool_call: dict, tool_signature: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Validates and converts arguments in the input dictionary to match the expected types.\n",
        "\n",
        "    Args:\n",
        "        tool_call (dict): A dictionary containing the arguments passed to the tool.\n",
        "        tool_signature (dict): The expected function signature and parameter types.\n",
        "\n",
        "    Returns:\n",
        "        dict: The tool call dictionary with the arguments converted to the correct types if necessary.\n",
        "    \"\"\"\n",
        "    properties = tool_signature[\"parameters\"][\"properties\"]\n",
        "\n",
        "    # TODO: This is overly simplified but enough for simple Tools.\n",
        "    type_mapping = {\n",
        "        \"int\": int,\n",
        "        \"str\": str,\n",
        "        \"bool\": bool,\n",
        "        \"float\": float,\n",
        "    }\n",
        "\n",
        "    for arg_name, arg_value in tool_call[\"arguments\"].items():\n",
        "        expected_type = properties[arg_name].get(\"type\")\n",
        "\n",
        "        if not isinstance(arg_value, type_mapping[expected_type]):\n",
        "            tool_call[\"arguments\"][arg_name] = type_mapping[expected_type](arg_value)\n",
        "\n",
        "    return tool_call\n",
        "\n",
        "\n",
        "class Tool:\n",
        "    \"\"\"\n",
        "    A class representing a tool that wraps a callable and its signature.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the tool (function).\n",
        "        fn (Callable): The function that the tool represents.\n",
        "        fn_signature (str): JSON string representation of the function's signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, fn: Callable, fn_signature: str):\n",
        "        self.name = name\n",
        "        self.fn = fn\n",
        "        self.fn_signature = fn_signature\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fn_signature\n",
        "\n",
        "    def run(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Executes the tool (function) with provided arguments.\n",
        "\n",
        "        Args:\n",
        "            **kwargs: Keyword arguments passed to the function.\n",
        "\n",
        "        Returns:\n",
        "            The result of the function call.\n",
        "        \"\"\"\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "\n",
        "def tool(fn: Callable):\n",
        "    \"\"\"\n",
        "    A decorator that wraps a function into a Tool object.\n",
        "\n",
        "    Args:\n",
        "        fn (Callable): The function to be wrapped.\n",
        "\n",
        "    Returns:\n",
        "        Tool: A Tool object containing the function, its name, and its signature.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper():\n",
        "        fn_signature = get_fn_signature(fn)\n",
        "        return Tool(\n",
        "            name=fn_signature.get(\"name\"), fn=fn, fn_signature=json.dumps(fn_signature)\n",
        "        )\n",
        "\n",
        "    return wrapper()"
      ],
      "metadata": {
        "id": "GMlqRhXxlnAG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Extraction util**"
      ],
      "metadata": {
        "id": "gXVly_dMmAEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TagContentResult:\n",
        "    \"\"\"\n",
        "    A data class to represent the result of extracting tag content.\n",
        "\n",
        "    Attributes:\n",
        "        content (List[str]): A list of strings containing the content found between the specified tags.\n",
        "        found (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "\n",
        "    content: list[str]\n",
        "    found: bool\n",
        "\n",
        "\n",
        "def extract_tag_content(text: str, tag: str) -> TagContentResult:\n",
        "    \"\"\"\n",
        "    Extracts all content enclosed by specified tags (e.g., , , etc.).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input string containing multiple potential tags.\n",
        "        tag (str): The name of the tag to search for (e.g., 'thought', 'response').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the following keys:\n",
        "            - 'content' (list): A list of strings containing the content found between the specified tags.\n",
        "            - 'found' (bool): A flag indicating whether any content was found for the given tag.\n",
        "    \"\"\"\n",
        "    # Build the regex pattern dynamically to find multiple occurrences of the tag\n",
        "    tag_pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
        "\n",
        "\n",
        "    # Use findall to capture all content between the specified tag\n",
        "    matched_contents = re.findall(tag_pattern, text, re.DOTALL)\n",
        "\n",
        "    # Return the dataclass instance with the result\n",
        "    return TagContentResult(\n",
        "        content=[content.strip() for content in matched_contents],\n",
        "        found=bool(matched_contents),\n",
        "    )"
      ],
      "metadata": {
        "id": "P4rB6oRxmCT4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Planning workflow** {react}"
      ],
      "metadata": {
        "id": "b9eannkTmHEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**System prompt**"
      ],
      "metadata": {
        "id": "GrfbB9LAmPqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the System Prompt as a constant\n",
        "REACT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You operate by running a loop with the following steps: Thought, Action, Observation.\n",
        "You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>, \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools / actions:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\n",
        "Example session:\n",
        "\n",
        "<question>What's the current temperature in Madrid?</question>\n",
        "<thought>I need to get the current weather in Madrid</thought>\n",
        "<tool_call>{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}</tool_call>\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "<observation>{0: {\"temperature\": 25, \"unit\": \"celsius\"}}</observation>\n",
        "\n",
        "You then output:\n",
        "\n",
        "<response>The current temperature in Madrid is 25 degrees Celsius</response>\n",
        "\n",
        "Additional constraints:\n",
        "\n",
        "- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with <response></response> tags.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gMVJYTRTmFBW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tools**"
      ],
      "metadata": {
        "id": "B8FpeCeCmbCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def sum_two_elements(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    Computes the sum of two integers.\n",
        "\n",
        "    Args:\n",
        "        a (int): The first integer to be summed.\n",
        "        b (int): The second integer to be summed.\n",
        "\n",
        "    Returns:\n",
        "        int: The sum of `a` and `b`.\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply_two_elements(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    Multiplies two integers.\n",
        "\n",
        "    Args:\n",
        "        a (int): The first integer to multiply.\n",
        "        b (int): The second integer to multiply.\n",
        "\n",
        "    Returns:\n",
        "        int: The product of `a` and `b`.\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def compute_log(x: int) -> float | str:\n",
        "    \"\"\"\n",
        "    Computes the logarithm of an integer `x` with an optional base.\n",
        "\n",
        "    Args:\n",
        "        x (int): The integer value for which the logarithm is computed. Must be greater than 0.\n",
        "\n",
        "    Returns:\n",
        "        float: The logarithm of `x` to the specified `base`.\n",
        "    \"\"\"\n",
        "    if x <= 0:\n",
        "        return \"Logarithm is undefined for values less than or equal to 0.\"\n",
        "\n",
        "    return math.log(x)\n",
        "\n",
        "\n",
        "available_tools = {\n",
        "    \"sum_two_elements\": sum_two_elements,\n",
        "    \"multiply_two_elements\": multiply_two_elements,\n",
        "    \"compute_log\": compute_log\n",
        "}"
      ],
      "metadata": {
        "id": "cLdWxcImmcEu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tool name: \", sum_two_elements.name)\n",
        "print(\"Tool signature: \", sum_two_elements.fn_signature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZazE-zMgmlCb",
        "outputId": "8e90c8f5-ee91-4642-a2a6-53a16f8ced44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool name:  sum_two_elements\n",
            "Tool signature:  {\"name\": \"sum_two_elements\", \"description\": \"\\n    Computes the sum of two integers.\\n\\n    Args:\\n        a (int): The first integer to be summed.\\n        b (int): The second integer to be summed.\\n\\n    Returns:\\n        int: The sum of `a` and `b`.\\n    \", \"parameters\": {\"properties\": {\"a\": {\"type\": \"int\"}, \"b\": {\"type\": \"int\"}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Concat and add to system prompt**"
      ],
      "metadata": {
        "id": "PUylBMmQm9dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools_signature = sum_two_elements.fn_signature + \",\\n\" + multiply_two_elements.fn_signature + \",\\n\" + compute_log.fn_signature\n"
      ],
      "metadata": {
        "id": "AeQx5VzInAJK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tools_signature)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fS4B22vnDBT",
        "outputId": "299f7539-0ca4-4f6f-ce5a-f1ba8974ef6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"sum_two_elements\", \"description\": \"\\n    Computes the sum of two integers.\\n\\n    Args:\\n        a (int): The first integer to be summed.\\n        b (int): The second integer to be summed.\\n\\n    Returns:\\n        int: The sum of `a` and `b`.\\n    \", \"parameters\": {\"properties\": {\"a\": {\"type\": \"int\"}, \"b\": {\"type\": \"int\"}}}},\n",
            "{\"name\": \"multiply_two_elements\", \"description\": \"\\n    Multiplies two integers.\\n\\n    Args:\\n        a (int): The first integer to multiply.\\n        b (int): The second integer to multiply.\\n\\n    Returns:\\n        int: The product of `a` and `b`.\\n    \", \"parameters\": {\"properties\": {\"a\": {\"type\": \"int\"}, \"b\": {\"type\": \"int\"}}}},\n",
            "{\"name\": \"compute_log\", \"description\": \"\\n    Computes the logarithm of an integer `x` with an optional base.\\n\\n    Args:\\n        x (int): The integer value for which the logarithm is computed. Must be greater than 0.\\n\\n    Returns:\\n        float: The logarithm of `x` to the specified `base`.\\n    \", \"parameters\": {\"properties\": {\"x\": {\"type\": \"int\"}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REACT_SYSTEM_PROMPT = REACT_SYSTEM_PROMPT % tools_signature\n"
      ],
      "metadata": {
        "id": "5Xg5SsjMnDXA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(REACT_SYSTEM_PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuBz8X17nFaF",
        "outputId": "ab999b9c-351b-40c6-d370-a0cdb10a349e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a function calling AI model. You operate by running a loop with the following steps: Thought, Action, Observation.\n",
            "You are provided with function signatures within <tools></tools> XML tags.\n",
            "You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\n",
            "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
            "\n",
            "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
            "\n",
            "<tool_call>\n",
            "{\"name\": <function-name>,\"arguments\": <args-dict>, \"id\": <monotonically-increasing-id>}\n",
            "</tool_call>\n",
            "\n",
            "Here are the available tools / actions:\n",
            "\n",
            "<tools>\n",
            "{\"name\": \"sum_two_elements\", \"description\": \"\\n    Computes the sum of two integers.\\n\\n    Args:\\n        a (int): The first integer to be summed.\\n        b (int): The second integer to be summed.\\n\\n    Returns:\\n        int: The sum of `a` and `b`.\\n    \", \"parameters\": {\"properties\": {\"a\": {\"type\": \"int\"}, \"b\": {\"type\": \"int\"}}}},\n",
            "{\"name\": \"multiply_two_elements\", \"description\": \"\\n    Multiplies two integers.\\n\\n    Args:\\n        a (int): The first integer to multiply.\\n        b (int): The second integer to multiply.\\n\\n    Returns:\\n        int: The product of `a` and `b`.\\n    \", \"parameters\": {\"properties\": {\"a\": {\"type\": \"int\"}, \"b\": {\"type\": \"int\"}}}},\n",
            "{\"name\": \"compute_log\", \"description\": \"\\n    Computes the logarithm of an integer `x` with an optional base.\\n\\n    Args:\\n        x (int): The integer value for which the logarithm is computed. Must be greater than 0.\\n\\n    Returns:\\n        float: The logarithm of `x` to the specified `base`.\\n    \", \"parameters\": {\"properties\": {\"x\": {\"type\": \"int\"}}}}\n",
            "</tools>\n",
            "\n",
            "Example session:\n",
            "\n",
            "<question>What's the current temperature in Madrid?</question>\n",
            "<thought>I need to get the current weather in Madrid</thought>\n",
            "<tool_call>{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}</tool_call>\n",
            "\n",
            "You will be called again with this:\n",
            "\n",
            "<observation>{0: {\"temperature\": 25, \"unit\": \"celsius\"}}</observation>\n",
            "\n",
            "You then output:\n",
            "\n",
            "<response>The current temperature in Madrid is 25 degrees Celsius</response>\n",
            "\n",
            "Additional constraints:\n",
            "\n",
            "- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with <response></response> tags.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ReAct loop**"
      ],
      "metadata": {
        "id": "69KsJKV3jTja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**First loop**"
      ],
      "metadata": {
        "id": "9r4eLTNMnLzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USER_QUESTION = \"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\"\n",
        "chat_history = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": REACT_SYSTEM_PROMPT\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"<question>{USER_QUESTION}</question>\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "FEKxvb6WnHXb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKJUl5s7nP9r",
        "outputId": "faf6adba-d2bb-4407-ed44-f12988fc3b70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<thought>To solve this problem, I need to follow the order of operations: first, sum 1234 and 5678, then multiply the result by 5, and finally compute the logarithm of this result.</thought>\n",
            "<tool_call>{\"name\": \"sum_two_elements\",\"arguments\": {\"a\": 1234, \"b\": 5678}, \"id\": 0}</tool_call>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": output\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "eAdZjFFlnP6V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Extarct tool call**"
      ],
      "metadata": {
        "id": "rmveJJK2nUJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = extract_tag_content(output, tag=\"tool_call\")\n"
      ],
      "metadata": {
        "id": "eczWbTlInP41"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erM24qM6nP2M",
        "outputId": "c6c92b79-9415-405d-99f7-97db93b604a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TagContentResult(content=['{\"name\": \"sum_two_elements\",\"arguments\": {\"a\": 1234, \"b\": 5678}, \"id\": 0}'], found=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call.content[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wShEUigAnqrA",
        "outputId": "02884538-6ba5-41c5-b46a-89a19e63e753"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"sum_two_elements\",\"arguments\": {\"a\": 1234, \"b\": 5678}, \"id\": 0}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = json.loads(tool_call.content[0])\n"
      ],
      "metadata": {
        "id": "cBXwp29YnYk8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n"
      ],
      "metadata": {
        "id": "YbdFaO7RnkVp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIBG6UuzsbqE",
        "outputId": "ebdd7528-c8f1-49d0-bbe6-c6e9449ca6d1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6912"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert tool_result == 1234 + 5678\n"
      ],
      "metadata": {
        "id": "driGZ3LMsb7I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"<observation>{tool_result}</observation>\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "abQZIY1AsgrV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2nd loop starts**"
      ],
      "metadata": {
        "id": "I2N6PUohjKyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3R2B7g4jEG9",
        "outputId": "53bf1024-ee4a-44f5-ddb4-96bf530b7f9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<thought>I have the sum of 1234 and 5678, which is 6912. Now, I need to multiply this result by 5.</thought>\n",
            "<tool_call>{\"name\": \"multiply_two_elements\",\"arguments\": {\"a\": 6912, \"b\": 5}, \"id\": 1}</tool_call>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": output\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "cMUqe_hzjJpP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Extract tool call**\n"
      ],
      "metadata": {
        "id": "QG7cip-zj-v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = extract_tag_content(output, tag=\"tool_call\")\n",
        "tool_call = json.loads(tool_call.content[0])\n",
        "tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n",
        "tool_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m25UFVK2j5uL",
        "outputId": "2f61af09-0012-4dbb-e9a3-1ae876679d35"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34560"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert tool_result == (1234 + 5678) * 5\n"
      ],
      "metadata": {
        "id": "cd0UhddRj91x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"<observation>{tool_result}</observation>\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "zpDoCZrPkEcl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**third loop**"
      ],
      "metadata": {
        "id": "jV2tLiQ8kRRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1TRYBaykMLH",
        "outputId": "b0e778af-c4d8-4860-a684-bcfd4f44aaff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<thought>I have the result of multiplying 6912 by 5, which is 34560. Now, I need to compute the logarithm of this result.</thought>\n",
            "<tool_call>{\"name\": \"compute_log\",\"arguments\": {\"x\": 34560}, \"id\": 2}</tool_call>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": output\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "qZk_4a-xkWE5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**extract tool call**"
      ],
      "metadata": {
        "id": "GOGL7PMZkdHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = extract_tag_content(output, tag=\"tool_call\")\n",
        "tool_call = json.loads(tool_call.content[0])\n",
        "tool_result = available_tools[tool_call[\"name\"]].run(**tool_call[\"arguments\"])\n",
        "tool_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEGnbgTjkf0C",
        "outputId": "a3eaf3b1-3369-4ae1-e169-c551e7fbe60d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.450452222917992"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert tool_result == math.log((1234 + 5678) * 5)\n"
      ],
      "metadata": {
        "id": "6ZYxoaG9kh5M"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"<observation>{tool_result}</observation>\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "sKdS1QeAkkVW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4th loop**"
      ],
      "metadata": {
        "id": "ognJMnpLkm2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SETwqNbakmMf",
        "outputId": "f1593116-7f4c-448d-90eb-704353a1e129"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<thought>I have the logarithm of 34560, which is approximately 10.45. Now, I can provide the final answer to the user's question.</thought>\n",
            "<response>The sum of 1234 and 5678 is 6912. Multiplying this result by 5 gives 34560. The logarithm of this result is approximately 10.45.</response>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**now we got a `<response>` which mean its the `final answer`**"
      ],
      "metadata": {
        "id": "hLWCkzsQkwvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Putting all together**"
      ],
      "metadata": {
        "id": "NIab9EYJk7wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Prompt**"
      ],
      "metadata": {
        "id": "n5kSd2TTlp_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -> dict:\n",
        "    \"\"\"\n",
        "    Builds a structured prompt that includes the role and content.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The actual content of the prompt.\n",
        "        role (str): The role of the speaker (e.g., user, assistant).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the structured prompt.\n",
        "    \"\"\"\n",
        "    if tag:\n",
        "        prompt = f\"<{tag}>{prompt}</{tag}>\"\n",
        "    return {\"role\": role, \"content\": prompt}"
      ],
      "metadata": {
        "id": "B3dFDAH5mxsb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REACT_SYSTEM_PROMPT = \"\"\"\n",
        "You operate by running a loop with the following steps: Thought, Action, Observation.\n",
        "You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>, \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools / actions:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\n",
        "Example session:\n",
        "\n",
        "<question>What's the current temperature in Madrid?</question>\n",
        "<thought>I need to get the current weather in Madrid</thought>\n",
        "<tool_call>{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}</tool_call>\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "<observation>{0: {\"temperature\": 25, \"unit\": \"celsius\"}}</observation>\n",
        "\n",
        "You then output:\n",
        "\n",
        "<response>The current temperature in Madrid is 25 degrees Celsius</response>\n",
        "\n",
        "Additional constraints:\n",
        "\n",
        "- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with <response></response> tags.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gTyd7oYHktz_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Completion from llm**"
      ],
      "metadata": {
        "id": "BDVUVAlanZ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def completions_create(client, messages: list, model: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a request to the client's `completions.create` method to interact with the language model.\n",
        "\n",
        "    Args:\n",
        "        client (Groq): The Groq client object\n",
        "        messages (list[dict]): A list of message objects containing chat history for the model.\n",
        "        model (str): The model to use for generating tool calls and responses.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the model's response.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(messages=messages, model=model)\n",
        "    return str(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "zWUu1UbEncxJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chat History**"
      ],
      "metadata": {
        "id": "BgrYKux0nPTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatHistory(list):\n",
        "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
        "        \"\"\"Initialise the queue with a fixed total length.\n",
        "\n",
        "        Args:\n",
        "            messages (list | None): A list of initial messages\n",
        "            total_length (int): The maximum number of messages the chat history can hold.\n",
        "        \"\"\"\n",
        "        if messages is None:\n",
        "            messages = []\n",
        "\n",
        "        super().__init__(messages)\n",
        "        self.total_length = total_length\n",
        "\n",
        "    def append(self, msg: str):\n",
        "        \"\"\"Add a message to the queue.\n",
        "\n",
        "        Args:\n",
        "            msg (str): The message to be added to the queue\n",
        "        \"\"\"\n",
        "        if len(self) == self.total_length:\n",
        "            self.pop(0)\n",
        "        super().append(msg)\n"
      ],
      "metadata": {
        "id": "mHvKmhTOnSAn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_chat_history(history: list, msg: str, role: str):\n",
        "    \"\"\"\n",
        "    Updates the chat history by appending the latest response.\n",
        "\n",
        "    Args:\n",
        "        history (list): The list representing the current chat history.\n",
        "        msg (str): The message to append.\n",
        "        role (str): The role type (e.g. 'user', 'assistant', 'system')\n",
        "    \"\"\"\n",
        "    history.append(build_prompt_structure(prompt=msg, role=role))\n"
      ],
      "metadata": {
        "id": "OFpic45FnTgk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ReAct \"planning\" Class**"
      ],
      "metadata": {
        "id": "8uYSgaqWlzFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactAgent:\n",
        "    \"\"\"\n",
        "    A class that represents an agent using the ReAct logic that interacts with tools to process\n",
        "    user inputs, make decisions, and execute tool calls. The agent can run interactive sessions,\n",
        "    collect tool signatures, and process multiple tool calls in a given round of interaction.\n",
        "\n",
        "    Attributes:\n",
        "        client (Groq): The Groq client used to handle model-based completions.\n",
        "        model (str): The name of the model used for generating responses. Default is \"llama-3.1-70b-versatile\".\n",
        "        tools (list[Tool]): A list of Tool instances available for execution.\n",
        "        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool instances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tools: Tool | list[Tool], model: str = \"llama-3.3-70b-versatile\",\n",
        "                  system_prompt: str = \"\") -> None:\n",
        "\n",
        "        self.client = Groq(api_key=groq_api_key)\n",
        "        self.model = model\n",
        "        self.system_prompt = system_prompt\n",
        "        self.tools = tools if isinstance(tools, list) else [tools]\n",
        "        self.tools_dict = {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def add_tool_signatures(self) -> str:\n",
        "        \"\"\"\n",
        "        Collects the function signatures of all available tools.\n",
        "\n",
        "        Returns:\n",
        "            str: A concatenated string of all tool function signatures in JSON format.\n",
        "        \"\"\"\n",
        "        return \"\".join([tool.fn_signature for tool in self.tools])\n",
        "\n",
        "    def process_tool_calls(self, tool_calls_content: list) -> dict:\n",
        "        \"\"\"\n",
        "        Processes each tool call, validates arguments, executes the tools, and collects results.\n",
        "\n",
        "        Args:\n",
        "            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n",
        "        \"\"\"\n",
        "        observations = {}\n",
        "        for tool_call_str in tool_calls_content:\n",
        "            tool_call = json.loads(tool_call_str)\n",
        "            tool_name = tool_call[\"name\"]\n",
        "            tool = self.tools_dict[tool_name]\n",
        "\n",
        "            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n",
        "\n",
        "            # Validate and execute the tool call\n",
        "            validated_tool_call = validate_arguments(\n",
        "                tool_call, json.loads(tool.fn_signature)\n",
        "            )\n",
        "            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n",
        "\n",
        "            result = tool.run(**validated_tool_call[\"arguments\"])\n",
        "            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n",
        "\n",
        "            # Store the result using the tool call ID\n",
        "            observations[validated_tool_call[\"id\"]] = result\n",
        "\n",
        "        return observations\n",
        "\n",
        "    def run(self, user_msg: str, max_rounds: int = 10) -> str:\n",
        "        \"\"\"\n",
        "        Executes a user interaction session, where the agent processes user input, generates responses,\n",
        "        handles tool calls, and updates chat history\n",
        "        until a final response is ready or the maximum number of rounds is reached.\n",
        "\n",
        "        Args:\n",
        "            user_msg (str): The user's input message to start the interaction.\n",
        "            max_rounds (int, optional): Maximum number of interaction rounds the agent should perform. Default is 10.\n",
        "\n",
        "        Returns:\n",
        "            str: The final response generated by the agent after processing user input and any tool calls.\n",
        "        \"\"\"\n",
        "        user_prompt = build_prompt_structure(\n",
        "            prompt=user_msg, role=\"user\", tag=\"question\"\n",
        "        )\n",
        "\n",
        "        # if there is avialable tools must add tool signatures to the prompt\n",
        "        if self.tools:\n",
        "            self.system_prompt += (\n",
        "                \"\\n\" + REACT_SYSTEM_PROMPT % self.add_tool_signatures()\n",
        "            )\n",
        "\n",
        "        # chat history starts with system prompt and user query only\n",
        "        chat_history = ChatHistory(\n",
        "            [\n",
        "                build_prompt_structure(\n",
        "                    prompt=self.system_prompt,\n",
        "                    role=\"system\",\n",
        "                ),\n",
        "                user_prompt,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if self.tools:\n",
        "            # Run the ReAct loop for max_rounds\n",
        "            for _ in range(max_rounds):\n",
        "\n",
        "                completion = completions_create(self.client, chat_history, self.model)\n",
        "\n",
        "                # if <response> tag --> final answer\n",
        "                response = extract_tag_content(str(completion), \"response\")\n",
        "                if response.found:\n",
        "                    return response.content[0]\n",
        "\n",
        "                thought = extract_tag_content(str(completion), \"thought\")\n",
        "                tool_calls = extract_tag_content(str(completion), \"tool_call\")\n",
        "\n",
        "                update_chat_history(chat_history, completion, \"assistant\")\n",
        "\n",
        "                print(Fore.MAGENTA + f\"\\nThought: {thought.content[0]}\")\n",
        "\n",
        "                if tool_calls.found:\n",
        "                    observations = self.process_tool_calls(tool_calls.content)\n",
        "                    print(Fore.BLUE + f\"\\nObservations: {observations}\")\n",
        "                    update_chat_history(chat_history, f\"{observations}\", \"user\")\n",
        "\n",
        "        return completions_create(self.client, chat_history, self.model)"
      ],
      "metadata": {
        "id": "cvl1RUvjluQY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test**"
      ],
      "metadata": {
        "id": "fsdghwiIobMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ReactAgent(tools=[sum_two_elements, multiply_two_elements, compute_log])\n"
      ],
      "metadata": {
        "id": "KzoMwqVOluNC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(user_msg=\"I want to calculate the sum of 1234 and 5678 and multiply the result by 5. Then, I want to take the logarithm of this result\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "rWS8bJzgluK0",
        "outputId": "46c0d160-293e-4996-a644-553144995436"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\n",
            "Thought: I need to calculate the sum of 1234 and 5678, then multiply the result by 5, and finally take the logarithm of this result. I will use the sum_two_elements, multiply_two_elements, and compute_log tools to achieve this.\n",
            "\u001b[32m\n",
            "Using Tool: sum_two_elements\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'sum_two_elements', 'arguments': {'a': 1234, 'b': 5678}, 'id': 0}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "6912\n",
            "\u001b[34m\n",
            "Observations: {0: 6912}\n",
            "\u001b[35m\n",
            "Thought: I have the sum of 1234 and 5678, which is 6912. Now, I need to multiply this result by 5.\n",
            "\u001b[32m\n",
            "Using Tool: multiply_two_elements\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'multiply_two_elements', 'arguments': {'a': 6912, 'b': 5}, 'id': 1}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "34560\n",
            "\u001b[34m\n",
            "Observations: {1: 34560}\n",
            "\u001b[35m\n",
            "Thought: I have the result of multiplying 6912 by 5, which is 34560. Now, I need to take the logarithm of this result.\n",
            "\u001b[32m\n",
            "Using Tool: compute_log\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'compute_log', 'arguments': {'x': 34560}, 'id': 2}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "10.450452222917992\n",
            "\u001b[34m\n",
            "Observations: {2: 10.450452222917992}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The sum of 1234 and 5678 is 6912. Multiplying 6912 by 5 results in 34560. The logarithm of 34560 is approximately 10.45.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}